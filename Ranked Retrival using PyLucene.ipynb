{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Retrival Using PyLucene\n",
    "## Abhisek Sarkar\n",
    "## Learnt from CS4201 Information Retrival and Web Search course work IISER Kolkata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import lucene\n",
    "from bs4 import BeautifulSoup\n",
    "from java.nio.file import Paths\n",
    "\n",
    "# Lucene-specific imports\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, TextField\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig\n",
    "from org.apache.lucene.store import NIOFSDirectory\n",
    "\n",
    "# Progress bar library\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperIndexer:\n",
    "    \"\"\"\n",
    "    This class indexes academic papers in PDF format using PyLucene.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index_path):\n",
    "        \"\"\"\n",
    "        Initializes the Lucene virtual machine and creates an IndexWriter object.\n",
    "\n",
    "        Args:\n",
    "            index_path (str): Path to the directory where the Lucene index will be stored.\n",
    "        \"\"\"\n",
    "        lucene.initVM()\n",
    "        self.writer = IndexWriter(\n",
    "            NIOFSDirectory(Paths.get(index_path)),\n",
    "            IndexWriterConfig(EnglishAnalyzer())\n",
    "        )\n",
    "\n",
    "    def index_papers(self, xml_directory, overwrite=False):\n",
    "        \"\"\"\n",
    "        Indexes all academic papers (in XML format) within a directory.\n",
    "\n",
    "        Args:\n",
    "            xml_directory (str): Path to the directory containing the XML files.\n",
    "            overwrite (bool, optional): If True, deletes existing index before indexing. Defaults to False.\n",
    "        \"\"\"\n",
    "        if overwrite:\n",
    "            self.writer.deleteAll()\n",
    "\n",
    "        for filename in tqdm(os.listdir(xml_directory)):\n",
    "            file_path = os.path.join(xml_directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'xml')\n",
    "\n",
    "                # Extract paper metadata\n",
    "                title = soup.find('title').text.strip()\n",
    "                abstract = soup.find('abstract').text.strip()\n",
    "                authors = []\n",
    "                for author in soup.find_all('author'):\n",
    "                    if not author.find('persName'):\n",
    "                        continue\n",
    "                    first_name = author.find('forename').text.strip() if author.find('forename') else ''\n",
    "                    last_name = author.find('surname').text.strip() if author.find('surname') else ''\n",
    "                    authors.append(f\"{first_name} {last_name}\".strip())\n",
    "                author_list = ', '.join(authors)\n",
    "                content = soup.body.get_text(separator=' ').strip()\n",
    "\n",
    "                # Create Lucene document and add fields\n",
    "                document = Document()\n",
    "                document.add(TextField('title', title, Field.Store.YES))\n",
    "                document.add(TextField('abstract', abstract, Field.Store.YES))\n",
    "                document.add(TextField('authors', author_list, Field.Store.YES))\n",
    "                document.add(TextField('content', content, Field.Store.YES))\n",
    "\n",
    "                self.writer.addDocument(document)\n",
    "\n",
    "        self.writer.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "class AcademicPaperParser:\n",
    "    \"\"\"\n",
    "    This class utilizes GrobidClient to parse academic papers in PDF format and generate XML files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_file_path=\"./config.json\"):\n",
    "        \"\"\"\n",
    "        Initializes a GrobidClient object using the provided configuration file path.\n",
    "\n",
    "        Args:\n",
    "            config_file_path (str, optional): Path to the Grobid client configuration file. Defaults to \"./config.json\".\n",
    "        \"\"\"\n",
    "        self.config_path = config_file_path\n",
    "        self.client = GrobidClient(config_path=self.config_path)\n",
    "\n",
    "    def parse_papers(self, input_directory, output_directory, service=\"processFulltextDocument\", number_of_files=10, verbosity=True, overwrite=True, generate_ids=False, consolidate_header=True, consolidate_citations=False, include_raw_citations=False, include_raw_affiliations=False, include_tei_coordinates=False, segment_sentences=False):\n",
    "        \"\"\"\n",
    "        Parses a specified number of PDF files from the input directory and generates corresponding XML files in the output directory.\n",
    "\n",
    "        Args:\n",
    "            input_directory (str): Path to the directory containing the PDF files.\n",
    "            output_directory (str): Path to the directory where the generated XML files will be stored.\n",
    "            service (str, optional): Grobid service to be used for parsing. Defaults to \"processFulltextDocument\".\n",
    "            number_of_files (int, optional): Number of PDF files to be parsed. Defaults to 10.\n",
    "            verbosity (bool, optional): Controls the logging output from GrobidClient. Defaults to True.\n",
    "            overwrite (bool, optional): If True, existing XML files in the output directory will be overwritten. Defaults to True.\n",
    "            generate_ids (bool, optional): If True, Grobid will generate unique IDs for entities in the parsed text. Defaults to False.\n",
    "            consolidate_header (bool, optional): If True, Grobid will consolidate header information. Defaults to True.\n",
    "            consolidate_citations (bool, optional): If True, Grobid will consolidate citation information. Defaults to False.\n",
    "            include_raw_citations (bool, optional): If True, Grobid will include raw citation data in the output. Defaults to False.\n",
    "            include_raw_affiliations (bool, optional): If True, Grobid will include raw affiliation data in the output. Defaults to False.\n",
    "            include_tei_coordinates (bool, optional): If True, Grobid will include TEI coordinates in the output. Defaults to False.\n",
    "            segment_sentences (bool, optional): If True, Grobid will segment the text into sentences. Defaults to False.\n",
    "        \"\"\"\n",
    "\n",
    "        self.client.process(\n",
    "            input_path=input_directory,\n",
    "            output=output_directory,\n",
    "            service=service,\n",
    "            n=number_of_files,\n",
    "            verbose=verbosity,\n",
    "            force=overwrite,\n",
    "            generateIDs=generate_ids,\n",
    "            consolidate_header=consolidate_header,\n",
    "            consolidate_citations=consolidate_citations,\n",
    "            include_raw_citations=include_raw_citations,\n",
    "            include_raw_affiliations=include_raw_affiliations,\n",
    "            teiCoordinates=include_tei_coordinates,\n",
    "            segment_sentences=segment_sentences\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = AcademicPaperParser()\n",
    "    parser.parse_papers(\"pdfs\", \"parsed_xmls\", verbose=True, overwrite=True, number_of_files=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "\n",
    "from java.nio.file import Paths\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.index import DirectoryReader\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search.similarities import BM25Similarity\n",
    "from org.apache.lucene.store import NIOFSDirectory\n",
    "\n",
    "\n",
    "class AcademicPaperSearcher:\n",
    "    \"\"\"\n",
    "    This class searches an indexed collection of academic papers using Lucene.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index_path):\n",
    "        \"\"\"\n",
    "        Initializes a Lucene IndexSearcher object for the specified index path.\n",
    "\n",
    "        Args:\n",
    "            index_path (str): Path to the Lucene index directory.\n",
    "        \"\"\"\n",
    "        lucene.initVM()\n",
    "        self.searcher = IndexSearcher(\n",
    "            DirectoryReader.open(NIOFSDirectory(Paths.get(index_path)))\n",
    "        )\n",
    "        self.searcher.setSimilarity(BM25Similarity())\n",
    "\n",
    "    def search(self, search_query, search_field, number_of_results=10):\n",
    "        \"\"\"\n",
    "        Searches the Lucene index for documents matching the query within a specific field.\n",
    "\n",
    "        Args:\n",
    "            search_query (str): The query string to be used for searching.\n",
    "            search_field (str): The field within the documents to search in.\n",
    "            number_of_results (int, optional): The maximum number of search results to return. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries containing information about the retrieved documents.\n",
    "        \"\"\"\n",
    "        parsed_query = QueryParser(search_field, EnglishAnalyzer()).parse(search_query)\n",
    "        top_hits = self.searcher.search(parsed_query, number_of_results).scoreDocs\n",
    "\n",
    "        results = []\n",
    "        for hit in top_hits:\n",
    "            document = self.searcher.doc(hit.doc)\n",
    "            results.append({\n",
    "                'title': document.get('title'),\n",
    "                'abstract': document.get('abstract'),\n",
    "                'authors': document.get('authors'),\n",
    "                'body': document.get('body'),\n",
    "                'score': hit.score\n",
    "            })\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize objects\n",
    "indexer = PaperIndexer()\n",
    "searcher = AcademicPaperSearcher(\"index\")\n",
    "\n",
    "# Parse PDFs and generate XMLs\n",
    "AcademicPaperParser().parse_papers(\"pdfs\", \"xmls\", verbose=False, overwrite=False, number_of_files=10)\n",
    "\n",
    "# Index the generated XML files\n",
    "indexer.index_papers(\"xmls\", overwrite=True)\n",
    "\n",
    "\n",
    "# Main search loop\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        query = input(\"Query ⇨ \")\n",
    "        if query == \"exit\":\n",
    "            break\n",
    "        field = input(\"Field ⇨ \")\n",
    "        results = searcher.search(query, field)\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"{i + 1}. {result['title']} | [{result['score']:.2f}]\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
